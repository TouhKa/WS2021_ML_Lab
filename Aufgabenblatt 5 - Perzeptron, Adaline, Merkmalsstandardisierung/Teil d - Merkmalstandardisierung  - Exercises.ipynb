{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil d - Merkmalstandardisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verbesserung durch Merkmalstandardisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methode der Standardisierung und Auswirkung auf die Algorithmen\n",
    "\n",
    "Viele Lernalgorithmen machen es erforderlich die Merkmale zu standardisieren um eine optimale Leistung zu erzielen. Die Algorithmen Perzeptron und Adaline gehören zu den vielen Algorithmen, die von einer Standardisierung profitieren.\n",
    "\n",
    "Im folgenden wird die Methode <i>Standardisierung</i> erläutert. Diese verleiht den Daten die Eigenschaften einer Standardnormalverteilung. Der Mittelwert jedes Merkmals beträgt 0, die Standardabweichung jeder Spalte beträgt 1. Um zum Beispiel das Merkmal j zu standardiesieren, wird der Mittelwert $\\mu$ der jeweiligen Stichprobe abgezogen und das Ergebnis durch die Standardabweichung $\\sigma$ dividiert. Das Standardisierungsverfahren wird auf alle Merkmale der Datenmenge angewendet.\n",
    "\n",
    "$x_j^{\\prime(i)} = \\frac{x_j^{(i)} - \\mu_j}{\\sigma_j}$. \n",
    "\n",
    "Die Standardisierung verbessert die Algorithmen, weil zum Auffinden einer guten/ optimalen Lösung (das globale Minimum der Straffunktion) weniger Schritte erforderlich sind. Folgende Abbildung stellen die Strafflächen einer zweidimensionalen Klassifizierungsaufgabe als Funktion der Gewichtungen dar.\n",
    "\n",
    "<img src=\"./Figures/Merkmalstandardisierung.png\" alt=\"drawing\" style=\"width:500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung\n",
    "\n",
    "Selektieren Sie exakt dieselben Daten des Iris-Datensatzes aus Teil a und nehmen Sie die Standardisierung vor. <br>\n",
    "\n",
    "Trainineren Sie den entweder den Perzeptron-Algorithmus oder den Adaline-Algorithmus aus Teil a mit den standardisierten Daten mit verschiedenen Parametern. <br>\n",
    "\n",
    "Vergleichen Sie die Ergebnisse zwischen den standardisierten Daten und den nicht-standardisierten Daten. Stellen Sie die verschiedenen Resultate dar.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auswahl der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement\n",
    "df = pd.read_csv(\"./Data/iris.data\", header=None, sep=\",\")\n",
    "\n",
    "# Auswahl von setosa und versicolor\n",
    "y = df.loc[((df[4].str.contains('setosa')) | (df[4].str.contains('versicolor')) ), 4].map({'Iris-setosa': 0, 'Iris-versicolor': 1})\n",
    "\n",
    "# Auswahl von Kelch- und Bluetenblattlaenge\n",
    "X = df.loc[y.index, [0, 2]]\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung der Standardisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    df_normalized = df.copy()\n",
    "    \n",
    "    for col_name in df.columns:\n",
    "\n",
    "        u = np.mean(df_normalized.loc[:,col_name])\n",
    "        var = np.var(df_normalized.loc[:, col_name])\n",
    "        o = np.sqrt(var)\n",
    "        print(\"{}:\\n---\\nMittelwert: {}\\nVarianz: {}\\nStandartabweichung: {}\".format(col_name, u, var, o))\n",
    "\n",
    "        for idx, val in enumerate(df_normalized.loc[:,col_name]):           \n",
    "            df_normalized.loc[idx, col_name] =  (val - u )/o\n",
    "\n",
    "        u_strich = 1/o*(u-u) # siehe wikipedia\n",
    "        var_strich = np.var(df_normalized[col_name])\n",
    "        o_strich = np.sqrt(var_strich)\n",
    "        print(\"Mittelwert' : {}\\nVarianz' : {}\\nStandartabweichung' : {}\\n\".format(u_strich, var_strich, o_strich))\n",
    "    return df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\n",
      "---\n",
      "Mittelwert: 5.471000000000001\n",
      "Varianz: 0.40765900000000016\n",
      "Standartabweichung: 0.6384817930058775\n",
      "Mittelwert' : 0.0\n",
      "Varianz' : 0.9999999999999994\n",
      "Standartabweichung' : 0.9999999999999997\n",
      "\n",
      "2:\n",
      "---\n",
      "Mittelwert: 2.8620000000000005\n",
      "Varianz: 2.0773560000000004\n",
      "Standartabweichung: 1.4413035766277695\n",
      "Mittelwert' : 0.0\n",
      "Varianz' : 0.9999999999999997\n",
      "Standartabweichung' : 0.9999999999999998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_std = normalize(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training und Visualisierung des Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    \n",
    "    def __init__(self, eta=0.01, epochs=10):\n",
    "        self.eta = eta \n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def gewichtete_summe(self, x):\n",
    "        print(x, self.w[1:])\n",
    "        weightedSum = self.w[1:].T.dot(x) + self.w[0]\n",
    "        print(f'Weighted Sum: {weightedSum}')\n",
    "        return weightedSum\n",
    "        \n",
    "    def heaviside(self, summe):\n",
    "        if summe >= 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        errors_per_Epoch = []\n",
    "        total_Errors = []\n",
    "        numberOfFeatures = X.shape[1]\n",
    "        weights = self.initWeights(numberOfFeatures)\n",
    "        self.w = np.insert(weights,0,[1],axis = 0)\n",
    "        print(f'Initial Weights: {self.w}')\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"Epoch {epoch}\")\n",
    "            for idx, x in enumerate(X.values): \n",
    "                y_pred = (self.heaviside(self.gewichtete_summe(x)))\n",
    "                errors_per_Epoch.append(self.calculateError(y[idx], y_pred))\n",
    "                for feature in range(numberOfFeatures):\n",
    "                    self.updateWeights(y[idx], y_pred, feature, x[feature])\n",
    "                    \n",
    "            total_Errors.append(np.array(errors_per_Epoch).sum())       \n",
    "            errors_per_Epoch = []         \n",
    "\n",
    "        return total_Errors\n",
    "\n",
    "    def initWeights(self, size):\n",
    "        mu, sigma = 0, 0.01\n",
    "        return np.random.normal(mu, sigma, size)\n",
    "    \n",
    "    def updateWeights(self, y_true, y_pred, weightIndex, xj):\n",
    "        print(f'Old Weight: {self.w[weightIndex+1]}')\n",
    "       \n",
    "        weightfactor = xj * (self.eta*(y_true - y_pred)) \n",
    "        self.w[weightIndex+1] += weightfactor\n",
    "        print(f'New Weight: {self.w[weightIndex+1]}')\n",
    "        \n",
    "    def calculateError(self, y_true, y_pred):\n",
    "        return abs(y_true - y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement\n",
    "list_of_epochs = [10,20,50,100]\n",
    "list_of_etas = [1,0.1, 0.01, 0.00001, 0.0000001]\n",
    "\n",
    "for epochs in list_of_epochs:\n",
    "    for eta in list_of_etas: \n",
    "    \n",
    "        perceptron = Perceptron(eta=eta, epochs=epochs)\n",
    "        error = perceptron.fit(X_std,y)\n",
    "        \n",
    "        plt.plot(range(epochs), error)\n",
    "        plt.ylabel(\"Sum of false classfication per epoch\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.title(\"Epochs \" + str(epochs) + \" Learning Rate\" + str(eta))\n",
    "        plt.savefig(\"./Figures/TeilD/\" + str(epochs) + \"_\" +  str(eta).replace(\".\",\"\") + \".png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
